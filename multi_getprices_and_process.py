# -*- coding: utf-8 -*-
"""MULTI_GetPrices_and_Process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17I9dUiBl3AefDVesq9y1GxPdGxTLFCLx
"""

import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import time
from google.colab import drive
import os  # Ensure os is imported
import requests  # Ensure requests is imported
import time  # Ensure time is imported


# Function to fetch the top coins by market cap
def get_top_coins_by_market_cap(n=200):
    url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {
        'vs_currency': 'usd',
        'order': 'market_cap_desc',
        'per_page': n,
        'page': 1,
        'sparkline': 'false'
    }
    headers = {
        "accept": "application/json",
        "x-cg-demo-api-key": "CG-ZZLFUQooopRkr47Z1yqNWKyP"
    }
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        coins = response.json()
        coin_ids = [coin['id'] for coin in coins]
        return coin_ids
    else:
        print(f"Error fetching top coins: {response.status_code}, {response.text}")
        return []

# Existing functions: fetch_crypto_data, preprocess_data, calculate_rsi, add_features, detect_anomalies_in_close, preprocess_and_add_indicators
# [Include your existing functions here]

def fetch_crypto_data(coin_id='bitcoin', vs_currency='usd', days='365', interval='daily'):
    API_KEY = "CG-ZZLFUQooopRkr47Z1yqNWKyP"
    headers = {
        "accept": "application/json",
        "x-cg-demo-api-key": API_KEY
    }
    url = f'https://api.coingecko.com/api/v3/coins/{coin_id}/market_chart'
    params = {
        'vs_currency': vs_currency,
        'days': days,
        'interval': interval
    }
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        historical_data = response.json()
        # Process data into a DataFrame
        prices = historical_data['prices']
        market_caps = historical_data['market_caps']
        total_volumes = historical_data['total_volumes']
        data = []
        for i in range(len(prices) - 1):
            timestamp = prices[i][0]
            open_price = prices[i][1]
            close_price = prices[i + 1][1]
            market_cap = market_caps[i][1]
            volume = total_volumes[i][1]
            date = datetime.utcfromtimestamp(timestamp / 1000).strftime('%Y-%m-%d')
            data.append({
                'date': date,
                'open': open_price,
                'close': close_price,
                'market_cap': market_cap,
                'volume': volume
            })
        df = pd.DataFrame(data)
        # Set 'date' as index
        df['date'] = pd.to_datetime(df['date'])
        df.set_index('date', inplace=True)
        return df
    else:
        print(f"Error fetching data for {coin_id}: {response.status_code}, {response.text}")
        return None

# Preprocess data
def preprocess_data(data):
    """Fill missing values in the data."""
    data['close'] = data['close'].ffill()
    return data

def calculate_rsi(prices, window):
    """
    Calculate the Relative Strength Index (RSI) for a given window.
    """
    delta = prices.diff()
    gain = delta.where(delta > 0, 0.0)
    loss = -delta.where(delta < 0, 0.0)
    avg_gain = gain.rolling(window=window, min_periods=window).mean()
    avg_loss = loss.rolling(window=window, min_periods=window).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def add_features(data, MA_windows=[7, 10, 14, 25, 50, 100], RSI_windows=[7, 10, 14, 25, 50, 100], horizons=[2, 5, 60, 120, 240, 360]):
    """Add moving averages, RSI, and other features to the dataset."""

    # Add Moving Averages (MA)
    for window in MA_windows:
        data[f'MA_{window}'] = data['close'].rolling(window=window).mean()

    # Add Relative Strength Index (RSI)
    for window in RSI_windows:
        data[f'RSI_{window}'] = calculate_rsi(data['close'], window)

    # Add Tomorrow's Close and Higher/Lower Binary Target
    data["Tomorrow"] = data["close"].shift(-1)
    data["Tomorrow_Higher"] = (data["Tomorrow"] > data["close"]).astype(int)

    # Add delta_close_tomorrowClose (Difference between tomorrow's close and today's close)
    data["delta_close_tomorrowClose"] = data["Tomorrow"] - data["close"]

    # Add percent increase between consecutive close prices (24-hour percent difference)
    data["percent_increase_close"] = ((data["close"] - data["close"].shift(1)) / data["close"].shift(1)) * 100

    # Add the ratio of delta_close_tomorrowClose and today's close
    data["delta_close_ratio"] = data["delta_close_tomorrowClose"] / data["close"]

    # Add historical close prices and their respective higher/lower binary targets
    for i in range(1, 7):
        data[f"Tomorrow_+{i}"] = data["close"].shift(-i-1)
        data[f"Tomorrow_+{i}_Higher"] = (data[f"Tomorrow_+{i}"] > data["close"]).astype(int)

    # Add horizon-based features (rolling averages and trend)
    for horizon in horizons:
        rolling_averages = data['close'].rolling(horizon).mean()
        data[f"Close_Ratio_{horizon}"] = data["close"] / rolling_averages
        data[f"Trend_{horizon}"] = data["Tomorrow_Higher"].shift(1).rolling(horizon).sum()

    # Handle NaN values selectively
    # For critical features, we can forward-fill or backward-fill
    data.ffill(inplace=True)
    data.bfill(inplace=True)

    # After filling, if any NaNs remain (e.g., at the very start), drop those rows
    data.dropna(inplace=True)

    return data


def detect_anomalies_in_close(data, window=20, z_thresh=3):
    """
    Detect anomalies in the 'close' column based on rolling mean and standard deviation (sigma rule).
    """
    rolling_mean = data['close'].rolling(window=window).mean()
    rolling_std = data['close'].rolling(window=window).std()
    z_scores = (data['close'] - rolling_mean) / rolling_std
    anomalies = data[np.abs(z_scores) > z_thresh]
    return anomalies

def preprocess_and_add_indicators(data):
    # Ensure we're working with a copy of the DataFrame to avoid SettingWithCopyWarning
    data = data.copy()

    data = preprocess_data(data)  # Fill missing values
    data = add_features(data)     # Add technical indicators

    # Detect anomalies in the 'close' column
    anomalies = detect_anomalies_in_close(data)
    if not anomalies.empty:
        print(f"\nAnomalies detected in 'close' for the dataset:")
        print(anomalies)

    # Replace anomalies with NaN using .loc to ensure proper assignment
    data.loc[anomalies.index, 'close'] = np.nan

    # Forward fill NaN values in 'close' column without using inplace
    data['close'] = data['close'].ffill()
    data['close'] = data['close'].bfill()  # Backward fill as well to handle NaNs at the start

    # After filling, check for any remaining NaN values
    if data['close'].isnull().any():
        print("NaN values remain in 'close' column after filling. Dropping these rows.")
        data.dropna(subset=['close'], inplace=True)

    return data

def main():
    import os  # Ensure os is imported
    import requests  # Ensure requests is imported
    import time  # Ensure time is imported
    from google.colab import drive  # Import drive for Google Colab

    # Mount Google Drive with force_remount=True to handle existing mounts
    try:
        drive.mount('/content/drive', force_remount=True)
        print("Google Drive mounted successfully.")
    except ValueError as ve:
        print(f"Drive mount failed: {ve}")
        return  # Exit the main function if mounting fails
    except Exception as e:
        print(f"An unexpected error occurred while mounting Google Drive: {e}")
        return  # Exit the main function if an unexpected error occurs

    # Get the top 200 coin_ids
    top_coin_ids = get_top_coins_by_market_cap(200)
    if not top_coin_ids:
        print("No coin IDs fetched. Exiting.")
        return

    print(f"Processing {len(top_coin_ids)} coins.")

    days = '365'
    interval = 'daily'

    request_count = 0  # Initialize request counter

    # Define the directory in Google Drive
    # For Google Colab, it's typically '/content/drive/My Drive/Colab Notebooks/Crypto_Data'
    # For local environments, adjust the path accordingly, e.g., 'G:/Crypto_Data' or '/Users/yourusername/Google Drive/Crypto_Data'
    save_directory = '/content/drive/My Drive/Colab Notebooks/Crypto_Data'

    # Create the directory if it doesn't exist
    try:
        os.makedirs(save_directory, exist_ok=True)
        print(f"Save directory is set to: {save_directory}")
    except Exception as e:
        print(f"Error creating directory {save_directory}: {e}")
        return  # Exit the main function if directory creation fails

    # Initialize an empty list to store processed coin_ids
    processed_coin_ids = []

    for coin_id in top_coin_ids:
        print(f"\nProcessing {coin_id}")
        df = fetch_crypto_data(coin_id=coin_id, days=days, interval=interval)
        if df is not None and not df.empty:
            # Process data
            processed_data = preprocess_and_add_indicators(df)
            # Check for NaN or infinite values and handle them
            processed_data.replace([np.inf, -np.inf], np.nan, inplace=True)
            processed_data.dropna(inplace=True)
            # Ensure DataFrame is not empty before saving
            if not processed_data.empty:
                # Construct the filename with full path
                filename = f'{coin_id}_{interval}_data.csv'
                file_path = os.path.join(save_directory, filename)
                try:
                    # Save to CSV in Google Drive
                    processed_data.to_csv(file_path)
                    print(f"Processed data saved to '{file_path}'")
                    # Append the coin_id to the list after successful save
                    processed_coin_ids.append(coin_id)
                    print(f"Appended {coin_id} to processed_coin_ids list.")
                except Exception as e:
                    print(f"Error saving data for {coin_id} to '{file_path}': {e}")
            else:
                print(f"Processed data for {coin_id} is empty after cleaning. No data was saved.")
        else:
            print(f"Failed to fetch data for {coin_id}.")

        # Increment request count
        request_count += 1

        # Check if we need to pause
        if request_count % 30 == 0:
            print("Pausing for 60 seconds to respect API rate limits...")
            time.sleep(60)
        else:
            # Sleep to respect rate limits between requests
            time.sleep(1)

    # Debugging: Print the list of processed_coin_ids
    print(f"\nTotal processed coins: {len(processed_coin_ids)}")
    print(f"Processed Coin IDs: {processed_coin_ids}")

    # After processing all coins, save the processed_coin_ids list to a text file
    if processed_coin_ids:
        coins_text_file = os.path.join(save_directory, 'processed_coins.txt')
        try:
            with open(coins_text_file, 'w') as f:
                for cid in processed_coin_ids:
                    f.write(f"{cid}\n")
            print(f"\nList of processed coin IDs saved to '{coins_text_file}'")
        except Exception as e:
            print(f"Error saving processed coin IDs to '{coins_text_file}': {e}")
    else:
        print("\nNo coins were processed successfully. 'processed_coins.txt' was not created.")

    print("\nAll coins have been processed.")






if __name__ == '__main__':
    main()