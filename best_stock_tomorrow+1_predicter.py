# -*- coding: utf-8 -*-
"""BEST_STOCK_TOMORROW+1_PREDICTER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wb6MT6fha9hDNO91oMrRBJ_h-MHs4o63

# DATA PREPERATION

Functions
"""

import os
import pandas as pd
from google.colab import drive
import numpy as np
import matplotlib.pyplot as plt

# Preprocess data
def preprocess_data(data):
    """Fill missing values in the data."""
    data['close'] = data['close'].ffill()  # Use forward fill to fill missing values
    return data

def calculate_rsi(prices, window):
    """
    Calculate the Relative Strength Index (RSI) for a given window.
    """
    delta = prices.diff()
    gain = pd.Series(0.0, index=prices.index)
    loss = pd.Series(0.0, index=prices.index)
    gain[1:] = delta.where(delta > 0, 0.0)[1:]
    loss[1:] = -delta.where(delta < 0, 0.0)[1:]
    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = loss.rolling(window=window, min_periods=1).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    rsi[(avg_gain == 0) & (avg_loss > 0)] = 0
    rsi[(avg_loss == 0) & (avg_gain > 0)] = 100
    return rsi

def add_features(data, MA_windows=[7, 10, 14, 25, 50, 100], RSI_windows=[7, 10, 14, 25, 50, 100], horizons=[2, 5, 60, 120, 240, 360]):
    """Add moving averages, RSI, and other features to the dataset."""

    # Add Moving Averages (MA)
    for window in MA_windows:
        data[f'MA_{window}'] = data['close'].rolling(window=window).mean()

    # Add Relative Strength Index (RSI)
    for window in RSI_windows:
        data[f'RSI_{window}'] = calculate_rsi(data['close'], window)

    # Add Tomorrow's Close and Higher/Lower Binary Target
    data["Tomorrow"] = data["close"].shift(-1)
    data["Tomorrow_Higher"] = (data["Tomorrow"] > data["close"]).astype(int)

    # Add delta_close_tomorrowClose (Difference between tomorrow's close and today's close)
    data["delta_close_tomorrowClose"] = data["Tomorrow"] - data["close"]

    # Add percent increase between consecutive close prices (24-hour percent difference)
    data["percent_increase_close"] = ((data["close"] - data["close"].shift(1)) / data["close"].shift(1)) * 100

    # Add the ratio of delta_close_tomorrowClose and today's close
    data["delta_close_ratio"] = data["delta_close_tomorrowClose"] / data["close"]

    # Add historical close prices and their respective higher/lower binary targets
    for i in range(1, 7):
        data["Tomorrow_+" + str(i)] = data["close"].shift(-i-1)
        data["Tomorrow_+" + str(i) + "_Higher"] = (data["Tomorrow_+" + str(i)] > data["close"]).astype(int)

    # Add horizon-based features (rolling averages and trend)
    for horizon in horizons:
        rolling_averages = data.rolling(horizon).mean()
        data[f"Close_Ratio_{horizon}"] = data["close"] / rolling_averages["close"]
        data[f"Trend_{horizon}"] = data["Tomorrow_Higher"].shift(1).rolling(horizon).sum()

    # Drop rows with NaN values generated by rolling operations
    data = data.dropna(subset=data.columns[data.columns != "Tomorrow"])

    # Drop NaN rows for Tomorrow_+i columns
    for i in range(1, 7):
        data = data.dropna(subset=data.columns[data.columns != "Tomorrow_+" + str(i)])

    return data


def preprocess_and_add_indicators(data, stock_name):
    data = preprocess_data(data)  # Fill missing values
    data = add_features(data)     # Add technical indicators

    # Detect anomalies in the 'close' column
    anomalies, anomaly_info = detect_anomalies_in_close(data, stock_name, window=20)

    if not anomalies.empty:
        print(f"\nSummary of detected anomalies in 'close' for stock: {stock_name}")
        print(anomalies)

    # Optionally, handle anomalies (e.g., forward fill or interpolation)
    rolling_mean = data['close'].rolling(window=20).mean()
    rolling_std = data['close'].rolling(window=20).std()
    z_scores = (data['close'] - rolling_mean) / rolling_std

    # Replace anomalies with NaN and forward fill them
    data['close'] = np.where(np.abs(z_scores) > 3, np.nan, data['close'])
    data['close'].ffill(inplace=True)

    return data

def load_stock_data_from_drive(directory, interval):
    """Load stock data CSV files from a specified Google Drive directory and filter by interval."""
    files = os.listdir(directory)
    stock_data = {}

    for file in files:
        if file.endswith('.csv') and file.startswith(interval):
            stock_name = file.split('.')[0].split('_')[1]
            file_path = os.path.join(directory, file)
            data = pd.read_csv(file_path)
            stock_data[stock_name] = data
            if debug:
                print(f"Loaded data for {stock_name} with {interval} interval from {file_path}")

    return stock_data

def save_combined_data_to_csv(data, interval, stock):
    """Save the combined data to a CSV file."""
    directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data'
    if not os.path.exists(directory):
        os.makedirs(directory)
    safe_stock = stock.replace('/', '_')
    filename = f'{interval}_{safe_stock}_combined_data.csv'
    file_path = os.path.join(directory, filename)
    data.to_csv(file_path)
    print(f'Saved combined data for {stock} {interval} interval at {file_path}')

def load_combined_data(interval, stock):
    """Load combined data for a given stock and interval from a CSV file."""
    directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data'
    safe_stock = stock.replace('/', '_')
    filename = f'{interval}_{safe_stock}_combined_data.csv'
    file_path = os.path.join(directory, filename)

    # Check if the file exists before loading
    if os.path.exists(file_path):
        print(f"Loading combined data for {stock} from {file_path}")
        data = pd.read_csv(file_path)

        # If time is an index or special column, handle it appropriately
        if 'time' in data.columns:
            data['time'] = pd.to_datetime(data['time'], unit='s')  # Convert UNIX back to datetime if needed
            data.set_index('time', inplace=True)

        return data
    else:
        print(f"No combined data found for {stock} at interval {interval}")
        return None

def check_for_nan(stock_data):
    """Check each dataframe in the stock_data dictionary for NaN values."""
    nan_found = False
    for stock_name, data in stock_data.items():
        nan_rows = data[data.isna().any(axis=1)]
        if not nan_rows.empty:
            nan_found = True
            print(f"\nNaN values found in stock: {stock_name}")
            print("Row indices with NaN values:", nan_rows.index.tolist())

            for idx in nan_rows.index:
                nan_columns = nan_rows.columns[nan_rows.loc[idx].isna()]
                print(f"Row {idx} has NaN in columns: {nan_columns.tolist()}")
                print(nan_rows.loc[idx])

    if not nan_found:
        print("No NaN values found in any of the loaded data.")

def drop_rows_with_nan(stock_data):
    """Drop rows with NaN values in any column from each dataframe in the stock_data dictionary."""
    for stock_name, data in stock_data.items():
        before_drop = data.shape[0]
        data.dropna(inplace=True)
        after_drop = data.shape[0]

        if before_drop != after_drop:
            print(f"Dropped {before_drop - after_drop} rows with NaNs from {stock_name}.")
        stock_data[stock_name] = data.copy()

    return stock_data

def needs_update(combined_data, required_features):
    return not set(required_features).issubset(set(combined_data.columns))

def detect_anomalies_in_close(data, stock_name, window=20, z_thresh=3, verbose=False):
    """
    Detect anomalies in the 'close' column based on rolling mean and standard deviation (sigma rule).

    Parameters:
    - data: DataFrame containing the stock data.
    - stock_name: The name of the stock (used in the plot title).
    - window: Window size for the rolling mean and standard deviation (default is 20).
    - z_thresh: Threshold for Z-score to consider an anomaly (default is 3 sigma).

    Returns:
    - anomalies: DataFrame containing rows with anomalies.
    - anomaly_info: List of dictionaries with details about the anomalies.
    """
    anomaly_info = []  # To store information about detected anomalies

    rolling_mean = data['close'].rolling(window=window).mean()
    rolling_std = data['close'].rolling(window=window).std()

    # Calculate Z-scores (distance from the rolling mean)
    z_scores = (data['close'] - rolling_mean) / rolling_std

    # Find rows where the Z-score exceeds the threshold
    anomalies = data[np.abs(z_scores) > z_thresh]

    if not anomalies.empty:
        if verbose:
            print(f"\nAnomalies detected in 'close' column for {stock_name}:")
            print(f"Number of anomalies: {len(anomalies)}")

        for idx, row in anomalies.iterrows():
            # Append information about the anomaly
            anomaly_info.append({
                'time': row.name,
                'value': row['close'],
                'z_score': z_scores.loc[idx]
            })

            if verbose:
                # Print the row with the anomaly
                print(f"\nAnomaly detected at time: {row.name}")
                print(f"Z-score: {z_scores.loc[idx]:.2f} | Close Value: {row['close']}")
                print("Full row data:")
                print(row)

        # Plot the anomalies along with the close prices
        plot_anomalies(data, anomalies, stock_name)

    return pd.DataFrame(anomaly_info), anomaly_info

def plot_anomalies(data, anomalies, stock_name):
    """
    Plot the 'close' prices and highlight anomalies on the graph, including stock name in the title.

    Parameters:
    - data: The full DataFrame containing stock data.
    - anomalies: DataFrame containing the detected anomalies.
    - stock_name: The name of the stock to display in the title.
    """
    plt.figure(figsize=(12, 6))

    # Plot the 'close' prices
    plt.plot(data.index, data['close'], label="Close Price", color="blue", lw=1)

    # Highlight anomalies with red markers
    plt.scatter(anomalies.index, anomalies['close'], color='red', label="Anomalies", zorder=5)

    plt.title(f"Close Price with Detected Anomalies for {stock_name}")
    plt.xlabel("Date")
    plt.ylabel("Close Price")
    plt.legend()

    # Display the plot
    plt.show()

"""Run Data Preperation"""

# Mount Google Drive to access files
drive.mount('/content/drive')

debug = True

combined_data_directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data'
use_existing_combined_data = False

def main_data_process():
    price_data_directory = '/content/drive/My Drive/Colab Notebooks/Price_Data/Trading_View/1D'
    intervals = ['1D']

    features = ['close', 'Volume', 'MA_10', 'MA_50', 'MA_100', 'RSI_7', 'RSI_14', 'RSI_25',
                'Tomorrow_Higher', 'Close_Ratio_2', 'Trend_2', 'Close_Ratio_5', 'Trend_5',
                'Close_Ratio_60', 'Trend_60', 'Close_Ratio_120', 'Trend_120', 'Close_Ratio_240', 'Trend_240',
                'Close_Ratio_360', 'Trend_360']

    interval_stocks = {}
    for interval in intervals:
        stock_data = load_stock_data_from_drive(price_data_directory, interval)

        for stock, data in stock_data.items():
            combined_data = None
            if use_existing_combined_data:
                combined_data = load_combined_data(interval, stock)
                if combined_data is not None and needs_update(combined_data, features):
                    print(f"Existing combined data for {stock} at interval {interval} lacks new features. Regenerating...")
                    combined_data = None

            if combined_data is None:
                data['time'] = pd.to_datetime(data['time'], unit='s')
                data.set_index('time', inplace=True)
                data = preprocess_and_add_indicators(data, stock)

                check_for_nan({stock: data})
                data_dict = drop_rows_with_nan({stock: data})
                data = data_dict[stock]

                if data.isnull().values.any() or np.isinf(data.values).any():
                    data.ffill(inplace=True)
                    data.replace([np.inf, -np.inf], 0, inplace=True)

                save_combined_data_to_csv(data, interval, stock)
                print(f"Shape of data is {data.shape}")
                combined_data = data

            if interval not in interval_stocks:
                interval_stocks[interval] = {}
            interval_stocks[interval][stock] = combined_data

    return interval_stocks

if __name__ == '__main__':
    main_data_process()

"""# PRICE PREDICTOR TRAINING

Dependencies
"""

!pip install keras-tuner

"""FUNCTIONS and RUN CODE"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
import json

class StockPricePredictor:
    def __init__(self, combined_data_directory, features, target, sequence_length=60, test_size=0.2, epochs=20, patience=10, batch_size=32):
        self.combined_data_directory = combined_data_directory
        self.sequence_length = sequence_length
        self.test_size = test_size
        self.epochs = epochs
        self.patience = patience
        self.batch_size = batch_size
        self.features = features
        self.target = target
        self.scalers = {}  # Store individual scalers for each stock
        self.target_scalers = {}  # Store individual target scalers for each stock
        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None
        self.stock_indices_train, self.stock_indices_test = None, None  # To track stock indices
        self.individual_models = {}  # Store models per feature
        self.feature_importances = {}  # Store feature importances
        self.stock_list = None  # Add this line to store the stock list



    def save_model(self, filepath):
        self.model.save(filepath)
        print(f"Model saved to {filepath}")

    def save_scalers(self, filepath):
        import pickle
        with open(filepath, 'wb') as f:
            pickle.dump({'scalers': self.scalers, 'target_scalers': self.target_scalers}, f)
        print(f"Scalers saved to {filepath}")

    def save_individual_models(self, directory_path):
        """Saves the individual models to the specified directory."""
        import os
        if not os.path.exists(directory_path):
            os.makedirs(directory_path)
        for feature, model in self.individual_models.items():
            model_path = os.path.join(directory_path, f'model_{feature}.h5')
            model.save(model_path, save_format='h5')
        print(f"Individual models saved to {directory_path}")

    def save_feature_importances(self, filepath):
        """Saves the feature importances to a JSON file."""
        with open(filepath, 'w') as f:
            json.dump(self.feature_importances, f)
        print(f"Feature importances saved to {filepath}")



    def load_combined_data(self, interval, stock):
        """Load combined data from CSV based on interval and stock symbol."""
        safe_stock = stock.replace('/', '_')
        filename = f'{interval}_{safe_stock}_combined_data.csv'
        file_path = os.path.join(self.combined_data_directory, filename)

        if os.path.exists(file_path):
            data = pd.read_csv(file_path)
            data.set_index('time', inplace=True)
            return data
        else:
            print(f"File not found at: {file_path}")
            return None

    def preprocess_data(self, stock_list, interval):
        self.stock_list = stock_list  # Store the stock list
        X_train_list, X_test_list = [], []
        y_train_list, y_test_list = [], []
        stock_indices_train, stock_indices_test = [], []

        for stock_index, stock in enumerate(stock_list):
            data = self.load_combined_data(interval, stock)
            if data is not None:
                # Initialize scalers for the current stock
                scaler = MinMaxScaler()
                target_scaler = MinMaxScaler()

                # Fit and transform the data using the stock's scaler
                scaled_features = scaler.fit_transform(data[self.features])
                scaled_target = target_scaler.fit_transform(data[[self.target]])

                # Store the scalers
                self.scalers[stock] = scaler
                self.target_scalers[stock] = target_scaler

                # Create sequences for the current stock
                X_sequences_stock, y_sequences_stock = [], []
                for i in range(len(scaled_features) - self.sequence_length):
                    X_sequences_stock.append(scaled_features[i:i + self.sequence_length])
                    y_sequences_stock.append(scaled_target[i + self.sequence_length])
                X_sequences_stock = np.array(X_sequences_stock)
                y_sequences_stock = np.array(y_sequences_stock)
                stock_indices_stock = np.full(len(X_sequences_stock), stock_index)

                # Split sequences for the current stock
                X_train_stock, X_test_stock, y_train_stock, y_test_stock, stock_indices_train_stock, stock_indices_test_stock = train_test_split(
                    X_sequences_stock, y_sequences_stock, stock_indices_stock, test_size=self.test_size, shuffle=False
                )

                # Append to the lists
                X_train_list.append(X_train_stock)
                X_test_list.append(X_test_stock)
                y_train_list.append(y_train_stock)
                y_test_list.append(y_test_stock)
                stock_indices_train.append(stock_indices_train_stock)
                stock_indices_test.append(stock_indices_test_stock)

        # Concatenate all stocks' data
        self.X_train = np.concatenate(X_train_list)
        self.X_test = np.concatenate(X_test_list)
        self.y_train = np.concatenate(y_train_list)
        self.y_test = np.concatenate(y_test_list)
        self.stock_indices_train = np.concatenate(stock_indices_train)
        self.stock_indices_test = np.concatenate(stock_indices_test)

        print("X_train shape:", self.X_train.shape)
        print("X_test shape:", self.X_test.shape)
        print("y_train shape:", self.y_train.shape)
        print("y_test shape:", self.y_test.shape)


    def build_baseline_model(self):
        """Builds and trains a baseline model using all features to compute feature importances."""
        model = Sequential()
        model.add(Input(shape=(self.sequence_length, len(self.features))))
        model.add(LSTM(units=100, return_sequences=True))
        model.add(Dropout(0.01))
        model.add(LSTM(units=100))
        model.add(Dropout(0.01))
        model.add(Dense(1))

        optimizer = Adam(learning_rate=0.002)
        model.compile(optimizer=optimizer, loss='mean_squared_error')

        # Early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=self.patience,
            restore_best_weights=True
        )

        # Train the model
        model.fit(
            self.X_train, self.y_train,
            epochs=self.epochs,
            batch_size=self.batch_size,
            validation_data=(self.X_test, self.y_test),
            callbacks=[early_stopping],
            verbose=1
        )

        self.baseline_model = model

    def compute_feature_importances(self, n_repeats=5):
        """Computes feature importances using permutation importance."""
        # Since we're using Keras models, we'll implement a custom permutation importance
        # Get predictions on the test set
        y_pred = self.baseline_model.predict(self.X_test)

        # Inverse transform predictions and actual values
        y_test_inverse = []
        y_pred_inverse = []
        for idx, stock in enumerate(stock_list):
            indices = np.where(self.stock_indices_test == idx)[0]
            if len(indices) > 0:
                scaler = self.target_scalers[stock]
                y_test_inverse.append(scaler.inverse_transform(self.y_test[indices]))
                y_pred_inverse.append(scaler.inverse_transform(y_pred[indices]))

        y_test_inverse = np.vstack(y_test_inverse)
        y_pred_inverse = np.vstack(y_pred_inverse)

        # Compute baseline MSE
        baseline_mse = mean_squared_error(y_test_inverse, y_pred_inverse)

        # Compute importances
        importances = {}
        for i, feature in enumerate(self.features):
            mse_scores = []
            for _ in range(n_repeats):
                X_test_permuted = self.X_test.copy()
                np.random.shuffle(X_test_permuted[:, :, i])
                y_pred_permuted = self.baseline_model.predict(X_test_permuted)

                # Inverse transform predictions
                y_pred_permuted_inverse = []
                for idx, stock in enumerate(stock_list):
                    indices = np.where(self.stock_indices_test == idx)[0]
                    if len(indices) > 0:
                        scaler = self.target_scalers[stock]
                        y_pred_permuted_inverse.append(scaler.inverse_transform(y_pred_permuted[indices]))

                y_pred_permuted_inverse = np.vstack(y_pred_permuted_inverse)

                mse_permuted = mean_squared_error(y_test_inverse, y_pred_permuted_inverse)
                mse_scores.append(mse_permuted)

            # Importance is the increase in MSE
            importances[feature] = np.mean(mse_scores) - baseline_mse

        # Normalize importances
        total_importance = sum(importances.values())
        for feature in importances:
            importances[feature] /= total_importance

        self.feature_importances = importances
        print("Feature Importances:")
        for feature, importance in importances.items():
            print(f"{feature}: {importance:.4f}")

    def train_individual_models(self):
        """Trains individual models for each feature."""
        for feature in self.features:
            print(f"Training model for feature: {feature}")
            feature_index = self.features.index(feature)
            # Prepare data with only the selected feature
            X_train_feature = self.X_train[:, :, feature_index].reshape(-1, self.sequence_length, 1)
            X_test_feature = self.X_test[:, :, feature_index].reshape(-1, self.sequence_length, 1)

            # Build and compile the model
            model = Sequential()
            model.add(Input(shape=(self.sequence_length, 1)))
            model.add(LSTM(units=100, return_sequences=True))
            model.add(Dropout(0.01))
            model.add(LSTM(units=100))
            model.add(Dropout(0.01))
            model.add(Dense(1))
            model.compile(optimizer='adam', loss='mean_squared_error')

            # Early stopping
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=self.patience,
                restore_best_weights=True
            )

            # Train the model
            model.fit(
                X_train_feature, self.y_train,
                epochs=self.epochs,
                batch_size=self.batch_size,
                validation_data=(X_test_feature, self.y_test),
                callbacks=[early_stopping],
                verbose=1
            )

            # Store the trained model
            self.individual_models[feature] = model

    def predict_and_plot_per_stock(self, stock_list):
        """Makes predictions for each stock separately and plots them."""
        for idx, stock in enumerate(stock_list):
            print(f"\nPredicting and plotting for stock: {stock}")
            # Get indices for this stock
            indices = np.where(self.stock_indices_test == idx)[0]
            if len(indices) == 0:
                print(f"No test data for stock: {stock}")
                continue
            # Extract the test data for this stock
            X_test_stock = self.X_test[indices]
            y_test_stock = self.y_test[indices]
            # Initialize weighted predictions
            weighted_predictions_stock = np.zeros(y_test_stock.shape)
            # Make predictions with individual models
            for feature, model in self.individual_models.items():
                feature_index = self.features.index(feature)
                X_test_feature = X_test_stock[:, :, feature_index].reshape(-1, self.sequence_length, 1)
                predictions = model.predict(X_test_feature)
                # Weight the predictions
                weight = self.feature_importances.get(feature, 0)
                weighted_predictions_stock += weight * predictions
            # Inverse transform predictions and actual values
            scaler = self.target_scalers[stock]
            weighted_predictions_inverse = scaler.inverse_transform(weighted_predictions_stock)
            y_test_inverse = scaler.inverse_transform(y_test_stock)
            # Evaluate
            mse = mean_squared_error(y_test_inverse, weighted_predictions_inverse)
            mae = mean_absolute_error(y_test_inverse, weighted_predictions_inverse)
            print(f"Stock: {stock}, MSE: {mse}, MAE: {mae}")
            # Plotting
            plt.figure(figsize=(12, 6))
            plt.plot(y_test_inverse, color='blue', label='Actual Prices')
            plt.plot(weighted_predictions_inverse, color='red', label='Predicted Prices')
            plt.title(f'Combined Model Predictions vs Actual for {stock}')
            plt.xlabel('Samples')
            plt.ylabel('Stock Price')
            plt.legend()
            plt.show()

    def plot_feature_importance(self, feature_importances, stock_name):
        """
        Plot the feature importances for a specific stock.

        Parameters:
        - feature_importances: Dictionary of features and their importance scores.
        - stock_name: Name of the stock.
        """
        features = list(feature_importances.keys())
        importances = list(feature_importances.values())

        plt.figure(figsize=(10, 6))
        plt.barh(features, importances, color='skyblue')
        plt.xlabel('Normalized Importance')
        plt.ylabel('Feature')
        plt.title(f'Feature Importances for {stock_name}')
        plt.gca().invert_yaxis()
        plt.show()

    def plot_feature_importances(self):
        """Plots the overall feature importances calculated by the baseline model."""
        features = list(self.feature_importances.keys())
        importances = list(self.feature_importances.values())

        plt.figure(figsize=(10, 6))
        plt.barh(features, importances, color='skyblue')
        plt.xlabel('Normalized Importance')
        plt.ylabel('Feature')
        plt.title('Overall Feature Importances')
        plt.gca().invert_yaxis()
        plt.show()


    def permutation_feature_importance(self, stock_name, n_repeats=5):
        """
        Calculate permutation feature importance for the specified stock.

        Parameters:
        - stock_name: The name of the stock to calculate importance for.
        - n_repeats: The number of times to shuffle each feature.

        Returns:
        - importances: A dictionary of features and their importance scores.
        """
        # Get the index of the stock
        stock_index = self.stock_list.index(stock_name)
        # Get indices for this stock in the test set
        indices = np.where(self.stock_indices_test == stock_index)[0]
        if len(indices) == 0:
            print(f"No test data for stock: {stock_name}")
            return None

        # Extract the test data for this stock
        X_test_stock = self.X_test[indices]
        y_test_stock = self.y_test[indices]

        # Get the baseline predictions and compute baseline MSE
        y_pred_baseline = self.baseline_model.predict(X_test_stock)
        scaler = self.target_scalers[stock_name]
        y_test_inverse = scaler.inverse_transform(y_test_stock)
        y_pred_baseline_inverse = scaler.inverse_transform(y_pred_baseline)
        baseline_mse = mean_squared_error(y_test_inverse, y_pred_baseline_inverse)

        importances = {}
        for i, feature in enumerate(self.features):
            mse_scores = []
            for _ in range(n_repeats):
                X_test_permuted = X_test_stock.copy()
                np.random.shuffle(X_test_permuted[:, :, i])
                y_pred_permuted = self.baseline_model.predict(X_test_permuted)
                y_pred_permuted_inverse = scaler.inverse_transform(y_pred_permuted)
                mse_permuted = mean_squared_error(y_test_inverse, y_pred_permuted_inverse)
                mse_scores.append(mse_permuted)
            # Importance is the increase in MSE
            importances[feature] = np.mean(mse_scores) - baseline_mse

        # Normalize importances
        total_importance = sum(importances.values())
        for feature in importances:
            importances[feature] /= total_importance

        # Sort features by importance
        sorted_importances = dict(sorted(importances.items(), key=lambda item: item[1], reverse=True))

        return sorted_importances

"""# Main Program Run"""

# Usage
if __name__ == "__main__":
    # Mount Google Drive
    drive.mount('/content/drive')

    # Set the data directory and interval
    combined_data_directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data'
    interval = '1D'  # Adjusted case to match your file

    # List of stock symbols to combine into one dataset

    if True:
        stock_list = [
            "ZECUSDT", "XRPUSDT", "XMRUSD", "XLMUSDT", "VETUSDT", "USOIL", "TRXUSDT",
            "SPX", "SOLUSDT", "SILVER", "SANDUSDT", "RTY", "OIL", "MSFT", "MINI",
            "LTCUSDT", "HBARUSDT", "GOOGL", "GOLD", "FTSE", "ETHUSDT", "DXY", "DOW",
            "DOTUSDT", "DOGEUSDT", "BNBUSDT", "BCHUSDT", "AVAXUSDT", "AMZN", "AMD",
            "ADAUSDT", "AAVEUSDT", "AAPL"]
    else:

        stock_list = [
            "ZECUSDT", "XRPUSDT", "XMRUSD", "XLMUSDT", "VETUSDT", "USOIL", "TRXUSDT",
            "SPX", "SOLUSDT", "SILVER"]

    #features = ['open', 'high', 'low', 'close', 'delta_close_tomorrowClose', 'Volume', 'MA_10', 'MA_50', 'MA_100', 'RSI_7', 'RSI_14', 'RSI_25']
    features_try= [
    'close', 'Volume','MA_7', 'MA_10', 'MA_14', 'MA_25', 'MA_50', 'MA_100',
    'RSI_7', 'RSI_10', 'RSI_14', 'RSI_25', 'RSI_50', 'RSI_100', 'delta_close_ratio',
    'Close_Ratio_2', 'Trend_2', 'Close_Ratio_5', 'Trend_5', 'Close_Ratio_60', 'Trend_60',
    'Close_Ratio_120', 'Trend_120', 'Close_Ratio_240', 'Trend_240', 'Close_Ratio_360', 'Trend_360'
    ]

    features = [
    'close', 'MA_7', 'MA_10', 'MA_14', 'delta_close_ratio'
    ]



    target = 'Tomorrow'
    target_1 = 'delta_close_tomorrowClose'
    target_2 = 'percent_increase_close'

    predictor = StockPricePredictor(
        combined_data_directory, features, target,
        sequence_length=60, test_size=0.15,
        epochs=10, patience=3, batch_size=32
    )

    # Preprocess all stocks data together
    predictor.preprocess_data(stock_list, interval)

    # Build and train the baseline model
    predictor.build_baseline_model()

    # Compute feature importances
    predictor.compute_feature_importances(n_repeats=1)

    # Plot feature importances
    predictor.plot_feature_importances()

    # Train individual models for each feature
    predictor.train_individual_models()

    # Save individual models
    models_directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data/individual_models'
    predictor.save_individual_models(models_directory)

    # Save feature importances
    feature_importances_filepath = '/content/drive/My Drive/Colab Notebooks/Combined_Data/feature_importances.json'
    predictor.save_feature_importances(feature_importances_filepath)

    # Save the scalers
    scalers_filepath = '/content/drive/My Drive/Colab Notebooks/Combined_Data/scalers.pkl'
    predictor.save_scalers(scalers_filepath)

    # Make predictions and plot for each stock separately
    predictor.predict_and_plot_per_stock(stock_list)

"""# FEATURE IMPORTANCE"""

stock_list = ["XRPUSDT", "VETUSDT", "SPX", "SILVER"]  # Add more stock symbols as needed

for stock in stock_list:
    print(f"Calculating feature importance for {stock}")
    feature_importances = predictor.permutation_feature_importance(stock, n_repeats=5)
    if feature_importances:
        predictor.plot_feature_importance(feature_importances, stock)

"""# POST TRAINED PREDICTIONS"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MinMaxScaler
import pickle
import json

class StockPredictor:
    def __init__(self, models_directory, scalers_path, feature_importances_path, sequence_length=60):
        self.sequence_length = sequence_length
        self.scalers = {}         # To store feature scalers for each stock
        self.target_scalers = {}  # To store target scalers for each stock
        self.individual_models = {}  # To store individual models per feature
        self.feature_importances = {}  # To store feature importances
        self.load_scalers(scalers_path)
        self.load_individual_models(models_directory)
        self.load_feature_importances(feature_importances_path)

    def load_scalers(self, filepath):
        """Load the scalers from a pickle file."""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        self.scalers = data['scalers']
        self.target_scalers = data['target_scalers']
        print(f"Scalers loaded from {filepath}")

    def load_individual_models(self, directory_path):
        """Loads individual models from the specified directory."""
        for filename in os.listdir(directory_path):
            if filename.endswith('.keras'):
                feature_name = filename[len('model_'):-len('.keras')]
                model_path = os.path.join(directory_path, filename)
                model = load_model(model_path)
                self.individual_models[feature_name] = model
        print(f"Individual models loaded from {directory_path}")

    def load_feature_importances(self, filepath):
        """Loads feature importances from a JSON file."""
        with open(filepath, 'r') as f:
            self.feature_importances = json.load(f)
        print(f"Feature importances loaded from {filepath}")

    def load_stock_data(self, stock, file_path):
        """Load stock data from a CSV file."""
        data = pd.read_csv(file_path)
        data['time'] = pd.to_datetime(data['time'])
        data.set_index('time', inplace=True)
        return data

    def preprocess_data(self, stock, data, features):
        """Scale the features and create the sequences needed for prediction."""
        scaler = self.scalers.get(stock)
        if scaler is None:
            raise ValueError(f"No scaler found for stock {stock}")
        scaled_data = scaler.transform(data[features])

        # Only create the latest sequence for prediction
        if len(scaled_data) < self.sequence_length:
            raise ValueError(f"Not enough data to create a sequence for stock {stock}")
        seq = scaled_data[-self.sequence_length:]
        return np.expand_dims(seq, axis=0)  # Shape (1, sequence_length, num_features)

    def batch_predict(self, stock_list, features, data_directory, interval):
        """Make batch predictions for all stocks using individual models."""
        predictions_dict = {}
        latest_close_values = {}

        for stock in stock_list:
            file_path = os.path.join(data_directory, f'{interval}_{stock}_combined_data.csv')
            data = self.load_stock_data(stock, file_path)

            # Get the latest close value
            latest_close_values[stock] = data['close'].iloc[-1]

            # Preprocess data
            sequences = self.preprocess_data(stock, data, features)

            # Initialize weighted prediction
            weighted_prediction = np.zeros((1, 1))

            # Make predictions with individual models
            for feature, model in self.individual_models.items():
                if feature not in features:
                    continue  # Skip features not used in this prediction
                feature_index = features.index(feature)
                X_input = sequences[:, :, feature_index].reshape(1, self.sequence_length, 1)
                prediction = model.predict(X_input)
                weight = self.feature_importances.get(feature, 0)
                weighted_prediction += weight * prediction

            # Inverse transform the prediction
            target_scaler = self.target_scalers.get(stock)
            if target_scaler is None:
                raise ValueError(f"No target scaler found for stock {stock}")
            prediction_inverse = target_scaler.inverse_transform(weighted_prediction)
            predictions_dict[stock] = {
                'prediction': prediction_inverse[0][0],
                'latest_close': latest_close_values[stock],
                'data': data  # Pass the data for plotting
            }

        return predictions_dict

    def plot_predictions(self, predictions_dict):
        """Plot the last 60 'close' prices for each stock and the predicted price as a red point."""
        for stock, info in predictions_dict.items():
            data = info['data']
            prediction = info['prediction']

            # Get the last 60 data points
            data_last_60 = data[-60:]

            plt.figure(figsize=(14, 7))
            plt.plot(data_last_60.index, data_last_60['close'], label=f"{stock} Close Prices")

            # Determine the x-coordinate for the predicted price
            last_date = data_last_60.index[-1]
            if isinstance(last_date, pd.Timestamp):
                # If the index is a datetime, add one day
                next_date = last_date + pd.Timedelta(days=1)
            else:
                # If the index is not a datetime, increment by 1
                next_date = last_date + 1

            plt.scatter(next_date, prediction, color='red', label='Predicted Price')
            plt.title(f"{stock} Close Prices and Predicted Price")
            plt.xlabel("Time")
            plt.ylabel("Close Price")
            plt.legend()
            plt.show()


# Example usage
if __name__ == "__main__":
    from google.colab import drive
    drive.mount('/content/drive')

    # Define paths and parameters
    models_directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data/individual_models'
    scalers_path = '/content/drive/My Drive/Colab Notebooks/Combined_Data/scalers.pkl'
    feature_importances_path = '/content/drive/My Drive/Colab Notebooks/Combined_Data/feature_importances.json'

    data_directory = '/content/drive/My Drive/Colab Notebooks/Combined_Data'
    interval = '1D'  # Adjusted case to match your file

    stock_list = ["XRPUSDT", "XMRUSD"]  # Example stock list
    features = [
    'close', 'MA_7', 'MA_10', 'MA_14', 'delta_close_ratio'
    ]
    sequence_length = 60  # Must match the sequence length used during training

    # Initialize the predictor
    predictor = StockPredictor(models_directory=models_directory,
                               scalers_path=scalers_path,
                               feature_importances_path=feature_importances_path,
                               sequence_length=sequence_length)

    # Batch predict
    predictions_dict = predictor.batch_predict(stock_list, features, data_directory, interval)

    # Output the predictions and corresponding close values
    for stock, info in predictions_dict.items():
        prediction = info['prediction']
        close_value = info['latest_close']
        print(f"Predicted 'tomorrow' price for {stock}: {prediction}")
        print(f"Latest 'close' value for {stock}: {close_value}")

    # Plot the close prices and predictions
    predictor.plot_predictions(predictions_dict)